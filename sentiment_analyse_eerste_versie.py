# -*- coding: utf-8 -*-
"""Sentiment analyse eerste versie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N4VXggBWpAZnBCPV21nPf0-qEzuyCwyg

deze code is een test gebaseerd op de spam classifier van: https://www.milindsoorya.com/blog/build-a-spam-classifier-in-python
"""

# Commented out IPython magic to ensure Python compatibility.

# %matplotlib inline
import matplotlib.pyplot as plt
import csv
import sklearn
import pickle
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve

"""de dataset die we hierbij gebruiken is 'tweets.csv, hierin wordt bij 27480 tweets een sentiment van 'negatief', 'neutraal' of 'positief' aangegeven."""

tweets = pd.read_csv('Tweets.csv')
tweets.head(8)

#alleen de belangrijke kolommen houden
tweets.drop(['textID', 'selected_text'], axis = 1)

# Import nltk pakket en Punkt Tokenizer model
import nltk
nltk.download("punkt")
import warnings
warnings.filterwarnings('ignore')

# Initialize variables to store positive, neutral, and negative messages
positive_words = ""
neutral_words = ""
negative_words = ""

# Filter out rows where 'text' is not NaN
tweets = tweets[pd.notna(tweets['text'])]

# Creating a corpus of positive messages
for val in tweets[tweets['sentiment'] == 'positive'].text:
    text = val.lower()
    tokens = nltk.word_tokenize(text)
    for words in tokens:
        positive_words = positive_words + words + ' '

# Creating a corpus of neutral messages
for val in tweets[tweets['sentiment'] == 'neutral'].text:
    text = val.lower()
    tokens = nltk.word_tokenize(text)
    for words in tokens:
        neutral_words = neutral_words + words + ' '

# Creating a corpus of negative messages
for val in tweets[tweets['sentiment'] == 'negative'].text:
    text = val.lower()
    tokens = nltk.word_tokenize(text)
    for words in tokens:
        negative_words = negative_words + words + ' '

"""In het voorbeeld worden wordclouds gemaakt, hierin kan je snel zien wat voor woorden er worden gebruikt in de dataset en of deze een beetje overeen komen met het toegewezen sentiment."""

positive_wordcloud = WordCloud(width=500, height=300).generate(positive_words)
neutral_wordcloud = WordCloud(width=500, height=300).generate(neutral_words)
negative_wordcloud = WordCloud(width=500, height=300).generate(negative_words)

#positive wordcloud maken
plt.figure( figsize=(10,8), facecolor='w')
plt.imshow(positive_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

#neutral wordcloud maken
plt.figure( figsize=(10,8), facecolor='w')
plt.imshow(neutral_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

#negative wordcloud maken
plt.figure( figsize=(10,8), facecolor='w')
plt.imshow(negative_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

tweets = tweets.replace(['negative','neutral','positive'],[-1,0, 1])
tweets.head(8)

"""In de volgende regels gebruiken we een pakket om 'stopwoorden' uit de zinnen te filteren, zo worden alleen de belangrijke woorden gehouden voor de sentiment analyse en neemt deze woorden als bijvoorbeeld 'I', 'you' en 'it' niet mee bij het berekenen van een sentimet. Deze woorden komen namelijk in elke zin wel voor."""

nltk.download('stopwords')

#remove the punctuations and stopwords
import string
def text_process(text):

    text = text.translate(str.maketrans('', '', string.punctuation))
    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]

    return " ".join(text)

tweets['text'] = tweets['text'].apply(text_process)
tweets.head()

text = pd.DataFrame(tweets['text'])
sentiment = pd.DataFrame(tweets['sentiment'])

## Counting how many times a word appears in the dataset

from collections import Counter

total_counts = Counter()
for i in range(len(text)):
    for word in text.values[i][0].split(" "):
        total_counts[word] += 1

print("Total words in data set: ", len(total_counts))

# Sorting in decreasing order (Word with highest frequency appears first)
vocab = sorted(total_counts, key=total_counts.get, reverse=True)
print(vocab[:60])

"""In de volgende regels zullen de woorden omgezet worden in 'vectors', ofwel in getallen. Dit wordt in machine learning gebruikt om woorden in te kunnen lezen. Hiermee wordt ook aangegeven hoe belangrijk een woord in een zin is."""

# Mapping from words to index

vocab_size = len(vocab)
word2idx = {}

# Text to Vector
def text_to_vector(text):
    word_vector = np.zeros(vocab_size)
    for word in text.split(" "):
        if word2idx.get(word) is None:
            continue
        else:
            word_vector[word2idx.get(word)] += 1
    return np.array(word_vector)

# Convert all titles to vectors
word_vectors = np.zeros((len(text), len(vocab)), dtype=np.int_)
for i, (_, text_) in enumerate(text.iterrows()):
    word_vectors[i] = text_to_vector(text_[0])

word_vectors.shape

#convert the text data into vectors
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(tweets['text'])
vectors.shape

features = vectors

X_train, X_test, y_train, y_test = train_test_split(features, tweets['sentiment'], test_size=0.15, random_state=111)

"""In de volgende code worden verschillende pakketen gedownload en vergeleken welke het beste werkt."""

#import sklearn packages for building classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

#initialize multiple classification models
svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier(n_neighbors=49)
mnb = MultinomialNB(alpha=0.2)
dtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=31, random_state=111)

#create a dictionary of variables and models
clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc}

#fit the data onto the models
def train(clf, features, targets):
    clf.fit(features, targets)

def predict(clf, features):
    return (clf.predict(features))

"""Hieronder kijken we welk model het beste werkt op de getrainde data."""

pred_scores_word_vectors = []
for k,v in clfs.items():
    train(v, X_train, y_train)
    pred = predict(v, X_test)
    pred_scores_word_vectors.append((k, [accuracy_score(y_test , pred)]))

pred_scores_word_vectors

"""'LR' heeft de hoogste score, deze zullen we gebuiken voor de sentiment analyse."""

#write functions to detect if the message is spam or not
def find(x):
    if x == 1:
        print ("Sentiment is positief")
    if x == 0:
        print ("Sentiment is neutraal")
    if x == -1:
        print ("sentiment is negatief")

"""Door in 'newtext' een zin of tweet in te voeren, wordt er een sentiment analyse gedaan."""

newtext = ["it is good"]
integers = vectorizer.transform(newtext)

x = lrc.predict(integers)
find(x)